{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOljCkqENLK23DGQVLNH6O/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day159.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning and Fine-Tuning"
      ],
      "metadata": {
        "id": "jQXsNAoKVwNr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "* Training deep learning models from scratch often requires vast amounts of data and computational resources, which may not always be feasible.\n",
        "* **Transfer learning** offers a solution by allowing you to leverage pre-trained models, which have already learned useful features from large datasets.\n",
        "* By transferring this knowledge to a new task, you can significantly reduce the time and data required to train a model.\n",
        "* In this section, we will explore the concept of transfer learning, how to use pre-trained models from libraries like `torchvision` and **Hugging Face Transformers**, and the strategies for fine-tuning models to suit domain-specific tasks.\n",
        "* We will also discuss the differences between **feature extraction** and **fine-tuning** strategies.\n",
        "\n",
        "\n",
        "## **Introduction to Transfer Learning**\n",
        "\n",
        "**Transfer learning** is a powerful technique in deep learning where a model developed for one task is reused as the starting point for another task. This approach is particularly useful when you have limited data for the new task but can access a model pre-trained on a large dataset.\n",
        "\n",
        "### **Concept of Transfer Learning**\n",
        "\n",
        " * **Definition:** Transfer learning involves taking a model trained on a large dataset (e.g., ImageNet for image classification) and adapting it to a new, related task by either using it as a fixed feature extractor or fine-tuning it on the new dataset.\n",
        "\n",
        "    * **Example:** Using a ResNet model pre-trained on ImageNet to classify medical images by fine-tuning the last few layers on a medical dataset.\n",
        "\n",
        "\n",
        "\n",
        "### **Benefits of Transfer Learning**\n",
        "\n",
        "* **Reduced Training Time:** Since the model has already learned useful features, the training process is faster compared to training from scratch.\n",
        "* **Improved Performance:** Transfer learning can lead to better performance on the target task, especially when the target dataset is small.\n",
        "* **Lower Data Requirements:** It allows effective model training even with limited labeled data.\n",
        "\n",
        "\n",
        "## **Using Pre-Trained Models from torchvision and Hugging Face Transformers**\n",
        "\n",
        "Pre-trained models are readily available through popular libraries like `torchvision` for computer vision tasks and **Hugging Face Transformers** for natural language processing. These models serve as a starting point for transfer learning.\n",
        "\n",
        "### **Pre-Trained Models in torchvision**\n",
        "\n",
        "* **Overview:** `torchvision` offers a variety of pre-trained models for tasks like image classification, object detection, and segmentation. These models are trained on large datasets like ImageNet.\n",
        "* **Example:** Loading a pre-trained ResNet model:\n",
        "\n",
        "```python\n",
        "import torchvision.models as models\n",
        "\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "```\n",
        "\n",
        "* **Applications:** You can use these models directly for inference or adapt them to your specific task through fine-tuning.\n",
        "\n",
        "### **Pre-Trained Models in Hugging Face Transformers**\n",
        "\n",
        "* **Overview:** Hugging Face Transformers provides access to a wide range of pre-trained models for NLP tasks such as text classification, named entity recognition, and text generation.\n",
        "* **Example:** Loading a pre-trained BERT model for text classification:\n",
        "\n",
        "```python\n",
        "from transformers import BertModel, BertTokenizer\n",
        "\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "```\n",
        "\n",
        "* **Applications:** These models can be fine-tuned for tasks like sentiment analysis, question answering, and machine translation.\n",
        "\n",
        "\n",
        "## **Fine-Tuning Models for Domain-Specific Tasks**\n",
        "\n",
        "Fine-tuning involves further training a pre-trained model on a specific task, allowing it to adapt to the nuances of the new dataset.\n",
        "\n",
        "### **Fine-Tuning Process**\n",
        "\n",
        "* **Overview:** Fine-tuning typically involves freezing the early layers of the model (which capture general features) and training the later layers on the new task.\n",
        "* **Example:** Fine-tuning a ResNet model on a medical imaging dataset by freezing the convolutional layers and training the fully connected layers.\n",
        "\n",
        "```python\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the final layer with a new one for the specific task\n",
        "resnet.fc = torch.nn.Linear(resnet.fc.in_features, num_classes)\n",
        "\n",
        "```\n",
        "\n",
        "### **Hyperparameter Tuning**\n",
        "\n",
        "During fine-tuning, it's essential to carefully select hyperparameters such as learning rate and batch size to avoid overfitting, especially if the new dataset is small.\n",
        "\n",
        "### **Challenges in Fine-Tuning**\n",
        "\n",
        "* **Overfitting:** Fine-tuning on a small dataset can lead to overfitting, where the model becomes too tailored to the training data and fails to generalize to new data.\n",
        "* **Catastrophic Forgetting:** If the fine-tuning process is too aggressive, the model might \"forget\" the useful features it learned from the original dataset, leading to degraded performance.\n",
        "\n",
        "\n",
        "\n",
        "## **Feature Extraction vs Fine-Tuning Strategies**\n",
        "\n",
        "When using transfer learning, there are two main strategies: **feature extraction** and **fine-tuning**. The choice depends on the size of your dataset and the similarity between the original and target tasks.\n",
        "\n",
        "### **Feature Extraction**\n",
        "\n",
        "* **Definition:** In feature extraction, the pre-trained model is used as a **fixed feature extractor**, meaning that the weights of the pre-trained model are frozen, and only the final classification layer is trained on the new dataset.\n",
        "* **Example:** Using a pre-trained ResNet model to extract features from images, then feeding those features into a new classifier trained on your specific dataset.\n",
        "\n",
        "```python\n",
        "for param in resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "```\n",
        "\n",
        "* **When to Use:** Feature extraction is ideal when you have a small dataset and the new task is similar to the original task the model was trained on.\n",
        "\n",
        "### **Fine-Tuning**\n",
        "\n",
        "* **Definition:** Fine-tuning involves unfreezing some or all of the layers of the pre-trained model and retraining them on the new dataset. This allows the model to adapt more specifically to the new task.\n",
        "* **Example:** Fine-tuning all layers of a BERT model on a domain-specific text classification task.\n",
        "\n",
        "```python\n",
        "model.train() # Set the model to training mode to fine-tune\n",
        "\n",
        "```\n",
        "\n",
        "* **When to Use:** Fine-tuning is preferred when the new task is somewhat different from the original task, or when you have a larger dataset that can support more extensive retraining.\n",
        "\n"
      ],
      "metadata": {
        "id": "haKOS6aqWdQp"
      }
    }
  ]
}