{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBa4SJ8tK+4ZGM+aqPcWSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day118.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple FeedForward Neural Network"
      ],
      "metadata": {
        "id": "iaMSv7hwRo1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Basic Neural Networks with TensorFlow\n",
        "\n",
        "# Building a simple feedforward neural network\n",
        "\n",
        "# 1. Import TensorFlow\n",
        "import tensorflow as tf\n",
        "\n",
        "# 2. Define the Architecture\n",
        "\n",
        "# Example architecture for a simple neural network\n",
        "input_size = 784  # (e.g., pixels in an image)\n",
        "hidden_size = 128 # Number of neurons in the hidden layer\n",
        "output_size = 10  # (e.g., digits 0-9)\n",
        "\n",
        "# 3. Specify the Input and Output Layers\n",
        "X = tf.keras.Input(shape=(input_size,), dtype=tf.float32, name='input_features')\n",
        "y = tf.keras.Input(shape=(output_size,), dtype=tf.float32, name='target_labels')\n",
        "\n",
        "# 4. Configure the Hidden Layers\n",
        "\n",
        "# Hidden layer\n",
        "hidden_layer = tf.keras.layers.Dense(units=hidden_size, activation=tf.nn.relu)(X)\n",
        "\n",
        "# 5. Specify the Output Layer\n",
        "\n",
        "# Output layer\n",
        "output_layer = tf.keras.layers.Dense(units=output_size)(hidden_layer)\n",
        "\n",
        "# Define the Keras Model\n",
        "model = tf.keras.Model(inputs=X, outputs=output_layer, name='feedforward_nn')\n",
        "\n",
        "# 6. Define the Loss Function\n",
        "class SoftmaxCrossEntropyWithLogitsLayer(tf.keras.layers.Layer):\n",
        "    def call(self, labels, logits):\n",
        "        return tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
        "\n",
        "# Wrap tf.reduce_mean in a Keras Layer to accept KerasTensors.\n",
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        return tf.reduce_mean(inputs)\n",
        "\n",
        "# 7. Select the Optimization Algorithm\n",
        "\n",
        "# Adam optimizer for gradient descent (TF2.x version)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# 8. Initialize Variables and Training Loop (TF2.x Eager Execution)\n",
        "\n",
        "# Define some dummy data for the example (as in previous code)\n",
        "num_epochs = 10\n",
        "input_data = tf.random.normal((100, input_size))\n",
        "target_labels = tf.one_hot(tf.random.uniform((100,), maxval=output_size, dtype=tf.int32), depth=output_size)\n",
        "\n",
        "# Build the model by passing dummy input to initialize weights\n",
        "_ = model(tf.zeros((1, input_size)))\n",
        "\n",
        "# Get trainable variables from the model after it's built\n",
        "trainable_vars = model.trainable_variables\n",
        "\n",
        "# For evaluation, use a Keras metric\n",
        "accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "test_data = tf.random.normal((10, input_size))\n",
        "test_labels = tf.one_hot(tf.random.uniform((10,), maxval=output_size, dtype=tf.int32), depth=output_size)\n",
        "\n",
        "print(\"Starting training\")\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        logits = model(input_data) # Use the Keras model directly\n",
        "\n",
        "        # Calculate loss using the custom Keras layers\n",
        "        cross_entropy = SoftmaxCrossEntropyWithLogitsLayer()(labels=target_labels, logits=logits)\n",
        "        current_loss = ReduceMeanLayer()(cross_entropy)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(current_loss, trainable_vars)\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    # Print training loss (using .numpy() to get scalar value from a TF tensor)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {current_loss.numpy()}\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "\n",
        "# 9. Evaluate the Model (Optional)\n",
        "\n",
        "# Perform evaluation in eager mode\n",
        "test_logits = model(test_data)\n",
        "\n",
        "# Update accuracy metric state\n",
        "accuracy_metric.update_state(y_true=test_labels, y_pred=tf.nn.softmax(test_logits))\n",
        "\n",
        "# Get the final accuracy result\n",
        "test_accuracy = accuracy_metric.result().numpy()\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaz1BBMbQQMO",
        "outputId": "21533575-d13e-4910-fdee-2cca253f0526"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training\n",
            "Epoch 1, Loss: 3.109400987625122\n",
            "Epoch 2, Loss: 2.392617702484131\n",
            "Epoch 3, Loss: 1.771527886390686\n",
            "Epoch 4, Loss: 1.2569451332092285\n",
            "Epoch 5, Loss: 0.857225239276886\n",
            "Epoch 6, Loss: 0.5674097537994385\n",
            "Epoch 7, Loss: 0.3691628575325012\n",
            "Epoch 8, Loss: 0.240563303232193\n",
            "Epoch 9, Loss: 0.15901751816272736\n",
            "Epoch 10, Loss: 0.1076737493276596\n",
            "Training finished.\n",
            "Test Accuracy: 0.1\n"
          ]
        }
      ]
    }
  ]
}