{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOImQDLCTFsYagVfEvbHME3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day119.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions"
      ],
      "metadata": {
        "id": "KagUob_6ULhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### • Linear Activation Function\n",
        "\n",
        "$f(x) = x$\n",
        "\n",
        "```python\n",
        "output = tf.keras.activations.linear(input)\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### • Sigmoid Activation Function\n",
        "\n",
        "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
        "\n",
        "```python\n",
        "output = tf.keras.activations.sigmoid(input)\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### • Hyperbolic Tangent (Tanh) Activation Function\n",
        "\n",
        "$f(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$\n",
        "\n",
        "```python\n",
        "output = tf.keras.activations.tanh(input)\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### • Rectified Linear Unit (ReLU) Activation Function\n",
        "\n",
        "$f(x) = \\max(0, x)$\n",
        "\n",
        "```python\n",
        "output = tf.keras.activations.relu(input)\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### • Leaky ReLU Activation Function\n",
        "\n",
        "$f(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\text{alpha} \\cdot x, & \\text{otherwise} \\end{cases}$\n",
        "\n",
        "```python\n",
        "output = tf.keras.layers.LeakyReLU(alpha=0.2)(input)\n",
        "\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### • Softmax Activation Function\n",
        "\n",
        "$f(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{j} e^{x_{j}}}$\n",
        "\n",
        "```python\n",
        "output = tf.keras.activations.softmax(input)\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "HfQxN6hrVi4n"
      }
    }
  ]
}