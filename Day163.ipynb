{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvcF0eKy6W41i39yraDX8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Data_Science/blob/main/Day163.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layers and Loss Functions"
      ],
      "metadata": {
        "id": "ja52Z5-psP0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Introduction**\n",
        "\n",
        "* While PyTorch provides a wide range of built-in layers, loss functions, and activation functions, there are times when you need to customize these components to fit specific needs or experiment with novel architectures.\n",
        "* This section covers how to create custom neural network layers and loss functions using `torch.nn.Module`, implement advanced activation functions, and apply regularization techniques to prevent overfitting.\n",
        "* By mastering these concepts, you'll gain greater flexibility in designing and optimizing deep learning models tailored to your specific tasks.\n",
        "\n",
        "\n",
        "\n",
        "## Creating Custom Neural Network Layers with torch.nn.Module\n",
        "\n",
        "Custom layers allow you to implement unique operations that are not available in PyTorch's standard library, providing more control over the model architecture.\n",
        "\n",
        "* **Subclassing torch.nn.Module**\n",
        "* **Overview:** To create a custom layer, subclass `torch.nn.Module` and implement the `__init__()` method to define the layer's parameters and the `forward()` method to define the computation.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(CustomLayer, self).__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Usage:** Once defined, the custom layer can be used like any other PyTorch layer in a neural network.\n",
        "* **Example:**\n",
        "```python\n",
        "model = nn.Sequential(\n",
        "    CustomLayer(10, 20),\n",
        "    nn.Linear(20, 10)\n",
        ")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Parameter Initialization**\n",
        "* **Custom Initialization:** You can customize the initialization of layer parameters using methods such as `torch.nn.init`.\n",
        "* **Example:**\n",
        "```python\n",
        "def __init__(self, in_features, out_features):\n",
        "    super(CustomLayer, self).__init__()\n",
        "    self.linear = nn.Linear(in_features, out_features)\n",
        "    torch.nn.init.xavier_uniform_(self.linear.weight)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Implementing Custom Loss Functions and Metrics\n",
        "\n",
        "Custom loss functions and metrics are essential when built-in options do not fit your specific problem or when you want to introduce novel evaluation criteria.\n",
        "\n",
        "* **Creating Custom Loss Functions**\n",
        "* **Overview:** To create a custom loss function, define a new function or subclass `torch.nn.Module` and implement the `forward()` method to calculate the loss.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomLoss, self).__init__()\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = F.mse_loss(output, target) + 0.1 * torch.mean(output)\n",
        "        return loss\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Usage:** The custom loss function can be used like any other loss function in your training loop.\n",
        "* **Example:**\n",
        "```python\n",
        "criterion = CustomLoss()\n",
        "loss = criterion(output, target)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Implementing Custom Metrics**\n",
        "* **Overview:** Metrics evaluate the performance of your model on validation or test data. You can create custom metrics by defining a function that compares predictions with the ground truth.\n",
        "* **Example:**\n",
        "```python\n",
        "def custom_accuracy(predictions, targets):\n",
        "    correct = (predictions.argmax(dim=1) == targets).float()\n",
        "    return correct.sum() / len(targets)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Usage:** Use custom metrics during model evaluation to gain insights beyond standard metrics like accuracy or loss.\n",
        "* **Example:**\n",
        "```python\n",
        "accuracy = custom_accuracy(predictions, targets)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Advanced Activation Functions: Swish, Mish, GELU\n",
        "\n",
        "Activation functions play a crucial role in introducing non-linearity into neural networks. While ReLU is the most common, advanced activation functions like Swish, Mish, and GELU can offer performance improvements in certain models.\n",
        "\n",
        "* **Swish**\n",
        "* **Overview:** Swish is an activation function defined as . It has been shown to perform better than ReLU in some deep networks.\n",
        "* **Example:**\n",
        "```python\n",
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Usage:** Replace ReLU with Swish in your model architecture where appropriate.\n",
        "\n",
        "\n",
        "* **Mish**\n",
        "* **Overview:** Mish is defined as , where . Mish has been found to improve the performance of various architectures, especially in computer vision tasks.\n",
        "* **Example:**\n",
        "```python\n",
        "class Mish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.tanh(F.softplus(x))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **GELU (Gaussian Error Linear Unit)**\n",
        "* **Overview:** GELU is defined as , where erf is the error function. It is used in models like BERT and has shown improved convergence properties in NLP tasks.\n",
        "* **Example:**\n",
        "```python\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return F.gelu(x)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Regularization Techniques: Dropout, Weight Decay\n",
        "\n",
        "Regularization is crucial for preventing overfitting, especially in complex models with a large number of parameters.\n",
        "\n",
        "* **Dropout**\n",
        "* **Overview:** Dropout is a regularization technique that randomly sets a fraction of the input units to zero during training, preventing the model from becoming too dependent on any particular node.\n",
        "* **Example:**\n",
        "```python\n",
        "class DropoutLayer(nn.Module):\n",
        "    def __init__(self, p=0.5):\n",
        "        super(DropoutLayer, self).__init__()\n",
        "        self.dropout = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dropout(x)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Weight Decay**\n",
        "* **Overview:** Weight decay (L2 regularization) penalizes large weights by adding a term to the loss function that is proportional to the sum of the squares of the weights.\n",
        "* **Usage:** Weight decay is applied during optimization and helps prevent overfitting by discouraging overly complex models.\n",
        "* **Example:**\n",
        "```python\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "# Or using Adam\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.01)\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KF1d8FFltSGS"
      }
    }
  ]
}