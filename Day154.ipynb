{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGX/CmeuJCquv+TM/Q8vMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day154.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autograd and Dynamic Compuation Graphs"
      ],
      "metadata": {
        "id": "IOh__m9V6yTv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Autograd and Dynamic Computation Graphs**\n",
        "\n",
        "**Introduction**\n",
        "\n",
        "* One of the core features that makes PyTorch a powerful framework for deep learning is its ability to automatically compute gradients.\n",
        "* This is facilitated by PyTorch’s autograd module, which enables automatic differentiation—essential for optimizing neural networks.\n",
        "* Additionally, PyTorch’s dynamic computation graphs allow for a more flexible and intuitive way to build and modify models on the fly.\n",
        "* In this section, we will explore these features in detail, providing you with a deep understanding of how PyTorch handles gradient computation and why dynamic computation graphs are a game-changer for deep learning research and development.\n",
        "\n",
        "\n",
        "**Introduction to Autograd**\n",
        "\n",
        "The autograd module is at the heart of PyTorch’s capability to automatically compute gradients for tensor operations. Gradients are essential in the optimization of neural networks as they provide the necessary information to update model parameters during training.\n",
        "\n",
        "* **What is Autograd?**\n",
        "* **Definition:** Autograd is PyTorch’s automatic differentiation engine that records operations performed on tensors to create a computation graph. This graph is used to calculate gradients during backpropagation.\n",
        "* **Importance:** Gradients are used to optimize the loss function, guiding the model in the right direction during training.\n",
        "\n",
        "\n",
        "* **How Autograd Works**\n",
        "* **Computation Graph:** When you perform operations on tensors, PyTorch dynamically constructs a computation graph that tracks the dependencies between tensors.\n",
        "* **Backward Pass:** During the backward pass, autograd traverses this graph to compute gradients for each tensor involved in the operations.\n",
        "\n",
        "\n",
        "\n",
        "# Dynamic Computation Graphs in PyTorch\n",
        "\n",
        "One of the key differences between PyTorch and other deep learning frameworks like TensorFlow (pre-2.0) is the use of dynamic computation graphs. This subsection explores how these graphs work and why they are advantageous.\n",
        "\n",
        "* **What are Dynamic Computation Graphs?**\n",
        "* **Definition:** Dynamic computation graphs, also known as define-by-run graphs, are built on the fly as operations are executed. Unlike static graphs, which are defined before running the model, dynamic graphs allow you to modify the graph structure during runtime.\n",
        "* **Flexibility:** This feature provides greater flexibility, especially in research and experimentation, where model architectures may need to be adjusted frequently.\n",
        "\n",
        "\n",
        "* **Advantages of Dynamic Computation Graphs**\n",
        "* **Ease of Use:** The define-by-run approach makes the code more intuitive and closer to standard Python code, reducing the learning curve for new users.\n",
        "* **Debugging:** Dynamic graphs are easier to debug because they are built step-by-step, allowing for the use of standard Python debugging tools.\n",
        "* **Conditionals and Loops:** PyTorch's dynamic graphs support conditionals and loops naturally, making it easier to implement complex model architectures such as RNNs and recursive models.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j4ehPIJI7KsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Introduction to Autograd\n",
        "\n",
        "**Key Concepts**\n",
        "\n",
        "* **Requires Grad:** By default, PyTorch does not compute gradients for tensors. To enable gradient computation, you must set `requires_grad=True` when creating a tensor.\n",
        "* **Example:**\n",
        "```python\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Gradients:** Once gradients are computed, they are stored in the `.grad` attribute of the tensor.\n",
        "\n",
        "\n",
        "## Automatic Differentiation with torch.autograd\n",
        "\n",
        "**Computing Gradients**\n",
        "\n",
        "* **Backward Method:** After performing some operations on a tensor, you can compute the gradient by calling the `backward()` method on the result.\n",
        "* **Example:**\n",
        "```python\n",
        "y = x * 2  # Example operation\n",
        "y.sum().backward()  # Compute gradients\n",
        "print(x.grad)  # Output the gradients\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Gradient Accumulation:** Gradients are accumulated into the `.grad` attribute by default, which means that calling `backward()` multiple times without resetting gradients can lead to incorrect results. You can reset gradients using `zero_grad()`.\n",
        "\n",
        "\n",
        "## Automatic Differentiation with torch.autograd (Continued)\n",
        "\n",
        "**Gradient Descent**\n",
        "\n",
        "* **Optimization:** The gradients computed by autograd are used to update model parameters using an optimization algorithm such as gradient descent.\n",
        "* **Example:**\n",
        "```python\n",
        "with torch.no_grad():\n",
        "    x -= learning_rate * x.grad\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Detaching Tensors:** Sometimes, you may need to detach a tensor from the computation graph to stop it from tracking gradients. This can be done using the `detach()` method.\n",
        "* **Example:**\n",
        "```python\n",
        "detached_tensor = x.detach()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Higher-Order Gradients\n",
        "\n",
        "* **Support for Higher-Order Gradients:** PyTorch also supports higher-order gradients, which are necessary for certain advanced techniques like meta-learning.\n",
        "* **Example:**\n",
        "```python\n",
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = x ** 2\n",
        "grad_outputs = torch.ones_like(x)\n",
        "gradients = torch.autograd.grad(y, x, grad_outputs=grad_outputs, create_graph=True)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "## Dynamic Computation Graphs in PyTorch\n",
        "\n",
        "**Practical Example of Dynamic Graphs**\n",
        "\n",
        "* **Example:** Consider a model where the number of layers is determined by an external condition, such as the length of an input sequence. With dynamic computation graphs, you can easily adjust the number of layers based on runtime conditions.\n",
        "* **Example:**\n",
        "```python\n",
        "layers = []\n",
        "for i in range(num_layers):\n",
        "    layers.append(torch.nn.Linear(10, 10))\n",
        "\n",
        "x = torch.randn(1, 10)\n",
        "for layer in layers:\n",
        "    x = torch.relu(layer(x))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "psj26Pjn70Uk"
      }
    }
  ]
}