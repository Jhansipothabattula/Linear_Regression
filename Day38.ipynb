{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmKUsa1b0XQfAQgUtEJf1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day38.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to XGBoost"
      ],
      "metadata": {
        "id": "XOjRLjVyvN7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview of XGBoost**\n",
        "\n",
        "- What is XGBoost?\n",
        "\n",
        "  - Advanced implementation of the Gradient Boosting algorithm designed for speed and perfomance\n",
        "\n",
        "  - It introduces various enhancements that make it faster, more efficient and capable of handling complex datasets\n",
        "\n",
        "- Improvements Over Traditional Gradient Boosting\n",
        "\n",
        "  - Speed\n",
        "\n",
        "  - Handling Missing Data\n",
        "\n",
        "  - Regularization\n",
        "\n",
        "  - Custom Loss Functions\n",
        "\n",
        "  - Tree Pruning"
      ],
      "metadata": {
        "id": "91oTbGw2vSgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key features of XGBoost**\n",
        "\n",
        "- Handling Missing Data\n",
        "\n",
        "  - Automatically assigns missing values to the branch that minimizes the loss function\n",
        "\n",
        "  - Reduces Preprocessing steps for datasets with missing values\n",
        "\n",
        "- Regularization\n",
        "\n",
        "  - Includes Penalties for overly complex models, reducing overfitting\n",
        "\n",
        "  - Hyperparameters\n",
        "\n",
        "    - Lambda:L2 Regularization term\n",
        "\n",
        "    - Alpha: L1 regularization term\n",
        "\n",
        "  - Parallel Processing\n",
        "\n",
        "   - Splits calculations for tree construction across multiple cores, significantly improving training time"
      ],
      "metadata": {
        "id": "ix075VABwL6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparameters inXGBoost and how to Tune them**\n",
        "\n",
        "- Key parameters\n",
        "\n",
        "  - Learning Rate(eta)\n",
        "\n",
        "    - Controls the contributions of each tree to the model\n",
        "\n",
        "    - Typical range: 0.01-0.3\n",
        "\n",
        "  - Number of Trees(n_estimators)\n",
        "\n",
        "    - Determines the number of boosting rounds\n",
        "\n",
        "    - larger values may improve perfomance but increase computation time\n",
        "\n",
        "  - Tree Depth(max_depth)\n",
        "\n",
        "    - Limits the depth of tree, balancing bias and variance\n",
        "\n",
        "    - Shallower trees generalize better, while deeper trees may overfit\n",
        "\n",
        "  - Subsample\n",
        "\n",
        "    - Fraction of data used to train each tree\n",
        "\n",
        "    - Helps reduce overfitting:typical range:0.5-1.0\n",
        "\n",
        "  - Colsamle_bytree\n",
        "\n",
        "    - fraction of features used for each tree split\n",
        "\n",
        "    - Typical range:0.5-1.0\n",
        "\n",
        "  - Regularization parameters: lambda and alpha control L2 and L1 regularization respectively"
      ],
      "metadata": {
        "id": "u2NrvIeqxXFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Train an XGBoost model on a dataset, tune Hyperparameters using cross-validation, and compare it's perfomance with a Gradient Boosting model**"
      ],
      "metadata": {
        "id": "3dDRxdOdy2o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display dataset info\n",
        "print(f\"Features: {data.feature_names}\")\n",
        "print(f\"classes: {data.target_names}\")\n",
        "\n",
        "# convert dataset to DMatrix\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Train XGBoost Model\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"max_depth\": 3,\n",
        "    \"eta\": 0.1,\n",
        "}\n",
        "\n",
        "xgb_model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# Predict\n",
        "y_pred = (xgb_model.predict(dtest)>0.5).astype(int)\n",
        "\n",
        "# Evaluate perfomance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"XGBoost Accuracy: \\n {accuracy}\")\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid ={\n",
        "    \"learning_rate\":[0.01, 0.1, 0.2],\n",
        "    \"n_estimators\":[50, 100, 200],\n",
        "    \"max_depth\":[3, 5, 7],\n",
        "    \"subsample\":[0.8, 1.0],\n",
        "    \"colsample_bytree\":[0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Initialize XGBoost classifier\n",
        "xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "\n",
        "# Perform GridSearch\n",
        "grid_search = GridSearchCV(estimator=xgb_clf, param_grid=param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display best parameters and score\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation accuracy: {grid_search.best_score_}\")\n",
        "\n",
        "# Evaluate Gradient Boosting Perfomance\n",
        "# Missing y_pred_gb for Gradient Boosting evaluation\n",
        "# accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "# print(f\"Gradient Boosting Accuracy: {accuracy_gb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT6v63J4zLqv",
        "outputId": "07662f73-a345-48c0-cc23-c79ec8317a93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "classes: ['malignant' 'benign']\n",
            "XGBoost Accuracy: \n",
            " 0.956140350877193\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94        43\n",
            "           1       0.96      0.97      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Best Cross-Validation accuracy: 0.9736263736263737\n"
          ]
        }
      ]
    }
  ]
}