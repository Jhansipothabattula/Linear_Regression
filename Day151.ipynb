{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORhFgsh46dZDqBVrM6NxG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day151.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to PyTorch"
      ],
      "metadata": {
        "id": "UTn8V5adx-v1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Introduction to PyTorch\n",
        "\n",
        "At its core, PyTorch is a **Tensors and Dynamic Neural Networks** library with strong GPU acceleration.\n",
        "\n",
        "* **Tensors:** Similar to NumPy's -dimensional arrays but with the ability to run on GPUs to speed up computing.\n",
        "* **Dynamic Computation Graphs:** Unlike frameworks that require you to define the entire network before running it, PyTorch builds the graph \"on the fly\" as the code executes.\n",
        "\n",
        "\n",
        "## 2. Key Features\n",
        "\n",
        "* **Dynamic Computational Graphs:** This is PyTorch’s \"killer feature.\" It allows you to change how the network behaves at runtime, making it much easier to debug and ideal for models with variable input lengths (like text).\n",
        "* **Autograd Module:** An automatic differentiation engine that calculates gradients (derivatives) automatically, which is essential for the backpropagation step in training neural networks.\n",
        "* **Pythonic Design:** PyTorch is designed to feel like a native part of Python. You can use standard Python debuggers (like `pdb`), loops, and conditional statements directly within your model logic.\n",
        "* **Distributed Training:** Native support for asynchronous execution and peer-to-peer communication, allowing you to scale models across multiple GPUs and nodes.\n",
        "\n",
        "\n",
        "## 3. Advantages\n",
        "\n",
        "* **Ease of Debugging:** Because it executes line-by-line (imperative mode), you can use standard Python print statements or debuggers to see exactly where a tensor's shape or value goes wrong.\n",
        "* **Research Dominance:** Because it’s so flexible, the vast majority of new AI research papers (over 75% at major conferences) use PyTorch. If you want to use the latest \"state-of-the-art\" model, it will likely be in PyTorch first.\n",
        "* **Strong Ecosystem:** It features specialized libraries like **Torchvision** (for images), **Torchtext** (for NLP), and **Torchaudio** (for sound).\n",
        "* **Production Ready:** While it started as a research tool, features like **TorchScript** and **TorchServe** now allow you to export and deploy models into high-performance C++ environments.\n",
        "\n",
        "\n",
        "## 4. History and Development\n",
        "\n",
        "* **2016:** PyTorch was created as an internship project by **Adam Paszke** under the guidance of **Soumith Chintala** at Facebook AI Research (FAIR). It was designed as a Python-based successor to the older **Torch** framework (which used the Lua language).\n",
        "* **2017:** The beta version was released to the public, quickly gaining a cult following among researchers.\n",
        "* **2018:** **Caffe2** (another Meta framework) was merged into PyTorch, combining research flexibility with production-level performance.\n",
        "* **2022:** Meta moved PyTorch to the **Linux Foundation**, forming the **PyTorch Foundation**. This ensured the project would be governed neutrally by a board including members from Google, Microsoft, Amazon, and Nvidia.\n",
        "\n",
        "\n",
        "## 5. PyTorch vs. Other Frameworks\n",
        "\n",
        "| Feature | PyTorch | TensorFlow | JAX |\n",
        "| --- | --- | --- | --- |\n",
        "| **Developer** | Meta (now Linux Foundation) | Google | Google |\n",
        "| **Graph Type** | **Dynamic** (Define-by-Run) | Static & Dynamic (via Eager) | Functional / JIT Compiled |\n",
        "| **Learning Curve** | Gentle (very Pythonic) | Steeper (more complex API) | Steep (requires functional programming) |\n",
        "| **Best For** | Research & Prototyping | Industrial Production & Mobile | High-performance Research |\n",
        "| **Market Share** | Leading in Academia | Leading in Enterprise/Mobile | Emerging in Research |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "vcMpv8Gb0ww6"
      }
    }
  ]
}