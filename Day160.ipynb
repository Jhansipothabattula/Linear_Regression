{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNarNWbVD/OPyT3c/ruvWM+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day160.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling Complex Data"
      ],
      "metadata": {
        "id": "7wK3O5eVXfpf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "* **Introduction**\n",
        "* In the realm of deep learning, dealing with complex data types such as **images, text, and time series** requires specialized techniques and careful preprocessing.\n",
        "* Whether you're enhancing image datasets with augmentation, preparing text data for natural language processing, or engineering features for time series forecasting, understanding how to handle these data types is crucial.\n",
        "* This section will cover advanced techniques for processing and preparing complex data for use in deep learning models.\n",
        "* We will explore image data augmentation methods, text preprocessing and tokenization strategies, and the unique challenges of handling time series data.\n",
        "\n",
        "\n",
        "\n",
        "## 2. Image Data Augmentation Techniques\n",
        "\n",
        "### Slide A: Overview and Random Cropping\n",
        "\n",
        "Data augmentation is a powerful technique used to artificially increase the size and diversity of an image dataset by applying random transformations. This helps prevent overfitting and improves the generalization ability of deep learning models.\n",
        "\n",
        "* **Random Cropping, Flipping, Rotation**\n",
        "  * **Random Cropping:** This technique involves randomly selecting a sub-region of an image and cropping it out. It helps the model become more robust to variations in object positioning within images.\n",
        "* **Example:**\n",
        "```python\n",
        "from torchvision.transforms import RandomCrop\n",
        "transform = RandomCrop(size=(224, 224))\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Flipping and Rotation\n",
        "\n",
        "* **Flipping:** Random horizontal or vertical flipping of images helps the model learn that the object's orientation is irrelevant to the classification.\n",
        "* **Example:**\n",
        "```python\n",
        "from torchvision.transforms import RandomHorizontalFlip\n",
        "transform = RandomHorizontalFlip(p=0.5)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Rotation:** Rotating images by a random degree helps the model become invariant to rotations, which is important for tasks where object orientation varies.\n",
        "* **Example:**\n",
        "```python\n",
        "from torchvision.transforms import RandomRotation\n",
        "transform = RandomRotation(degrees=45)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "###  Color and Brightness\n",
        "\n",
        "* **Color Jittering, Brightness/Contrast Adjustments**\n",
        "* **Color Jittering:** This technique randomly changes the brightness, contrast, saturation, and hue of an image, simulating different lighting conditions and making the model more robust to such variations.\n",
        "* **Example:**\n",
        "```python\n",
        "from torchvision.transforms import ColorJitter\n",
        "transform = ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Brightness/Contrast Adjustments:** Similar to color jittering, this method focuses specifically on altering the brightness and contrast of images.\n",
        "* **Example:**\n",
        "```python\n",
        "from torchvision.transforms import AdjustBrightness, AdjustContrast\n",
        "transform = AdjustBrightness(brightness_factor=0.3)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "## 3. Text Data Preprocessing and Tokenization\n",
        "\n",
        "### Tokenization Methods\n",
        "\n",
        "Text data requires specific preprocessing steps to convert raw text into a format that can be fed into deep learning models. Tokenization, in particular, is a key step in transforming text into tokens that represent the smallest units of meaning.\n",
        "\n",
        "* **Tokenization Methods: Word-Level, Character-Level**\n",
        "* **Word-Level Tokenization:** This method splits text into words, treating each word as a separate token. It's the most common form of tokenization and is often used in tasks like text classification and sentiment analysis.\n",
        "* **Example:**\n",
        "```python\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(\"This is an example sentence.\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Character-Level Tokenization:** This approach breaks text down into individual characters, making it useful for tasks like text generation or language modeling, where finer granularity is required.\n",
        "* **Example:**\n",
        "```python\n",
        "tokens = list(\"This is an example sentence.\")\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Sequences of Variable Length\n",
        "\n",
        "* **Handling Sequences of Variable Length**\n",
        "* **Padding:** Since neural networks require inputs of the same length, shorter sequences are often padded with a special token (e.g., zeros) to match the length of the longest sequence in the batch.\n",
        "* **Example:**\n",
        "```python\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "padded_sequences = pad_sequences(sequences, maxlen=100, padding='post')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Truncation:** If sequences are too long, they might be truncated to a maximum length to reduce computational load and prevent memory issues.\n",
        "* **Example:**\n",
        "```python\n",
        "truncated_sequences = pad_sequences(sequences, maxlen=100, truncating='post')\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Handling Long Sequences:** For very long sequences, advanced techniques like attention mechanisms (as in Transformers) can be used to focus on the most relevant parts of the sequence, reducing the need for padding or truncation.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Time Series Data Handling\n",
        "\n",
        "###  Architectures\n",
        "\n",
        "Time series data presents unique challenges because of its sequential nature and temporal dependencies. Handling this type of data effectively is key for tasks like forecasting, anomaly detection, and temporal pattern recognition.\n",
        "\n",
        "* **Temporal Convolutions and Recurrent Architectures**\n",
        "* **Temporal Convolutions:** Convolutional layers can be adapted to process time series data by applying filters over temporal windows, capturing patterns over time.\n",
        "* **Example:** Temporal Convolutional Networks (TCNs) apply causal convolutions to ensure that the model doesn't violate the sequence order by incorporating future information.\n",
        "* **Recurrent Architectures:** Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) networks are specifically designed for sequential data, maintaining a memory of past inputs through hidden states.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch.nn as nn\n",
        "rnn = nn.LSTM(input_size=10, hidden_size=50, num_layers=2)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Feature Engineering\n",
        "\n",
        "* **Feature Engineering for Time Series Forecasting**\n",
        "* **Lag Features:** Lag features are created by shifting the time series data by one or more time steps, allowing the model to capture temporal dependencies.\n",
        "* **Example:** Creating a lagged version of a time series to predict the next value based on previous values.\n",
        "* **Rolling Statistics:** Calculating rolling means, variances, and other statistics over a moving window helps in capturing trends and patterns over time.\n",
        "* **Example:**\n",
        "```python\n",
        "data['rolling_mean'] = data['value'].rolling(window=5).mean()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Seasonality and Trends:** Identifying and modeling seasonal patterns (e.g., daily, weekly, monthly) and trends in the data is crucial for accurate forecasting.\n",
        "* **Example:** Decomposing a time series into its trend, seasonality, and residual components.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ib9XahTqYICX"
      }
    }
  ]
}