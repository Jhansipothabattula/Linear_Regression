{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEhr+o+vhBw+ZnV3s/hZf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Q-Learning(DQN)"
      ],
      "metadata": {
        "id": "o81SLyzLWY7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Q-Networks (DQN)**\n",
        "\n",
        "Deep Q-Networks (DQN) is a reinforcement learning algorithm that combines Q-Learning with deep neural networks. It uses a neural network to approximate the Q-values for each action in a given state, allowing it to handle environments with high-dimensional and continuous state spaces. DQN uses experience replay (storing past experiences and training on random batches) and a target network to stabilize training"
      ],
      "metadata": {
        "id": "ATlyXPfPW5so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the DQN model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.linear1 = nn.Linear(input_size, 64)\n",
        "        self.linear2 = nn.Linear(64, 32)\n",
        "        self.linear3 = nn.Linear(32, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.linear1(x))\n",
        "        x = torch.relu(self.linear2(x))\n",
        "        return self.linear3(x)\n",
        "\n",
        "# Hyperparameters\n",
        "env_name = 'CartPole-v1'\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "buffer_size = 10000\n",
        "batch_size = 32\n",
        "epsilon = 0.1\n",
        "target_update_frequency = 100\n",
        "\n",
        "# Initialize environment and DQN\n",
        "env = gym.make(env_name) # gymnasium.make is now used\n",
        "input_size = env.observation_space.shape[0]\n",
        "output_size = env.action_space.n\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "policy_net = DQN(input_size, output_size).to(device)\n",
        "target_net = DQN(input_size, output_size).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Experience replay buffer\n",
        "replay_buffer = []\n",
        "\n",
        "def train(num_episodes):\n",
        "    step_count = 0  # Initialize step counter\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        # Modified: Unpack both observation and info from env.reset() as per Gymnasium API\n",
        "        state, _ = env.reset()\n",
        "        state = np.array(state)  # Ensure 'state' is a numpy array\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # Epsilon-greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "              action = env.action_space.sample() # Exploration\n",
        "            else:\n",
        "              with torch.no_grad():\n",
        "                action = policy_net(torch.tensor(state, dtype = torch.float, device=device)).argmax().item()\n",
        "\n",
        "            # Modified: Changed env.step() to match Gymnasium API (5 return values)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated # 'done' combines 'terminated' and 'truncated'\n",
        "\n",
        "            next_state = np.array(next_state)\n",
        "            total_reward += reward\n",
        "\n",
        "            # Store experience in replay buffer\n",
        "            replay_buffer.append((state, action, reward, next_state, done))\n",
        "            if len(replay_buffer) > buffer_size:\n",
        "                replay_buffer.pop(0)\n",
        "\n",
        "            # Update current state\n",
        "            state = next_state\n",
        "\n",
        "            # Sample a batch from the replay buffer\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                batch = random.sample(replay_buffer, batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                # Convert to tensors and move to device - Optimized for performance\n",
        "                states = torch.tensor(np.array(states), dtype=torch.float, device=device)\n",
        "                actions = torch.tensor(np.array(actions), dtype=torch.long, device=device)\n",
        "                rewards = torch.tensor(np.array(rewards), dtype=torch.float, device=device)\n",
        "                next_states = torch.tensor(np.array(next_states), dtype=torch.float, device=device)\n",
        "                dones = torch.tensor(np.array(dones), dtype=torch.float, device=device)\n",
        "\n",
        "                # Compute Q-values and target Q-values\n",
        "                current_q_values = policy_net(states).gather(1, actions.unsqueeze(1))\n",
        "                next_q_values = target_net(next_states).max(1)[0].detach()\n",
        "                target_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
        "\n",
        "                # Compute loss and update policy network\n",
        "                loss = criterion(current_q_values, target_q_values.unsqueeze(1))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Update target network periodically\n",
        "                step_count += 1\n",
        "                if step_count % target_update_frequency == 0:\n",
        "                    target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    print(f\"Episode: {episode}, Total reward: {total_reward}\")\n",
        "\n",
        "# Train the agent\n",
        "train(num_episodes=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8d7e78-211c-476b-aa90-98b6e5352f2c",
        "id": "-8xjyDrTyU5W"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 99, Total reward: 140.0\n"
          ]
        }
      ]
    }
  ]
}