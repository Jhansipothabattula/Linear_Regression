{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6IJAnMj4XJJVj8+3z9Qi3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day37.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting and Gradient Boosting"
      ],
      "metadata": {
        "id": "vHM_dHtRjfVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept of Boosting**\n",
        "\n",
        "- What is Boosting?\n",
        "\n",
        "  - Ensemble Technique that sequentially combines weak learners to form a strong learner\n",
        "\n",
        "  - Each subsequent model focuses on correcting the errors made by previous models\n",
        "\n",
        "- How does Boosting Differ from Bagging?\n",
        "\n",
        "| Feature      | Bagging    | Boosting     |\n",
        "|:----------|:----------:|----------:|\n",
        "| Approach        | Trains models independently on booststrapped subsets          | Trains models Sequentially to correct errors         |\n",
        "| Purpose        | Reduces variance by averaging predictions         | Reduces bias by focusing on difficult cases         |\n",
        "| Examples         | Random Forest          | Gradient Boosting, Adaboost        |\n"
      ],
      "metadata": {
        "id": "X5E7viAijl3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**\n",
        "\n",
        "- what is Gradient Boosting?\n",
        "\n",
        "  - Boosting Algorithm that builds sequentially by minimizes a loss function using Gradient Descent\n",
        "\n",
        "  - Iteratively adds weak learners to improve overall model perfomance\n",
        "\n",
        "- How Gradient Boosting Works\n",
        "\n",
        "  - Initialize Model: start with a simple model, often predicting the mean of the target variable\n",
        "\n",
        "  - Compute residuals: Calculate the difference between the actual and predicted values\n",
        "\n",
        "  - Fit weak Learner: Train a weak model to predict the residuals\n",
        "\n",
        "  - Update prediction: Add the predictions of the weak learner to the overall model\n",
        "\n",
        "  - Repeat: Continue adding weak learners until the desired number of iterations or a stopping criterion is reached\n",
        "\n"
      ],
      "metadata": {
        "id": "Hq4EGEyyljRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Boosting**\n",
        "\n",
        "- key Parameters in Gradient Boosting\n",
        "\n",
        "  - Learning rate\n",
        "\n",
        "    - Determines the contribution of each weak learner\n",
        "\n",
        "    - Smaller values reduce overfitting but require more itertaions\n",
        "\n",
        "  - Number of Estimators\n",
        "\n",
        "    - The Number of weak learners(trees)added sequentially\n",
        "\n",
        "    - Larger values improve learning but increase computation time\n",
        "\n",
        "  - Regualrization\n",
        "\n",
        "    - Techniques like limiting tree depth or adding penalties to prevent overfitting"
      ],
      "metadata": {
        "id": "uO03Mpu3n4Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding the key parameters**\n",
        "\n",
        "- Learning Rate(learning_rate)\n",
        "\n",
        "  - Lower values improve model perfomance by reducing overfitting but require more iterations\n",
        "\n",
        "  - Typical Range: 0.01 to 0.3\n",
        "\n",
        "- Number of estimators(n_estimators)\n",
        "\n",
        "  - Represents the number of trees added to the ensemble\n",
        "\n",
        "  - Larger values can improve pefomance but risk overfitting\n",
        "\n",
        "- Tree Depth(max_depth)\n",
        "\n",
        "  - Limits the complexity of individual trees\n",
        "\n",
        "  - Shallower trees generalize better but might underfit"
      ],
      "metadata": {
        "id": "mLomy1LPoska"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Train and Evaluate a Gradient Boosting model on a dataset, tune key parameters, and compare it's perfomance with a Random Forest model**"
      ],
      "metadata": {
        "id": "vkQqcOVPpptq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load Dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display dataset information\n",
        "print(f\"Features: {data.feature_names}\")\n",
        "print(f\"Classes: {data.target_names}\")\n",
        "\n",
        "# Train Gradient Boosting model\n",
        "gb_model = GradientBoostingClassifier(random_state=42)\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "\n",
        "# Evaluate perfomance\n",
        "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
        "print(f\"Gradient Boosting Accuracy: \\n{accuracy_gb:.2f}\")\n",
        "print(f\"Classification Report: \\n\", classification_report(y_test, y_pred_gb))\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"learning_rate\": [0.01, 0.1, 0.2], # Corrected parameter name\n",
        "    \"n_estimators\":[50, 100, 200],\n",
        "    \"max_depth\":[3, 5, 7]\n",
        "}\n",
        "\n",
        "# Perform Grid Search\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=GradientBoostingClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Display best parameters and score\n",
        "print(f\"Best Parameters: \\n{grid_search.best_params_}\")\n",
        "print(f\"Best Cross Validation Accuracy: \\n{grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f\"Random Forest Accuracy: \\n{accuracy_rf:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHRk4Vxpp6oq",
        "outputId": "7cabb2b9-bab3-4211-b1e2-026d406ff3f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n",
            "Classes: ['malignant' 'benign']\n",
            "Gradient Boosting Accuracy: \n",
            "0.96\n",
            "Classification Report: \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.93      0.94        43\n",
            "           1       0.96      0.97      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n",
            "Best Parameters: \n",
            "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200}\n",
            "Best Cross Validation Accuracy: \n",
            "0.96\n",
            "Random Forest Accuracy: \n",
            "0.96\n"
          ]
        }
      ]
    }
  ]
}