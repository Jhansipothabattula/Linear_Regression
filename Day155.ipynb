{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPW7JjJ+kNwVUecNz1Foqg3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jhansipothabattula/Machine_Learning/blob/main/Day155.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Simple Neural Network"
      ],
      "metadata": {
        "id": "ZeYdsz0j8v20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Introduction\n",
        "\n",
        "* Neural networks are the backbone of modern deep learning.\n",
        "* They consist of layers of interconnected neurons, which are designed to automatically learn patterns from data.\n",
        "* Building and training neural networks is a core skill in deep learning, and PyTorch makes it straightforward with its modular design.\n",
        "* In this section, we will explore the fundamental components of a neural network, how to construct neural networks using PyTorch’s `torch.nn.Module`, and the essential steps involved in training them.\n",
        "* We will also cover the crucial concepts of loss functions and optimizers, which play a key role in the learning process.\n",
        "\n",
        "\n",
        "# Anatomy of a Neural Network\n",
        "\n",
        "To build and understand neural networks, it’s important to grasp their basic structure and how the different components interact with each other.\n",
        "\n",
        "* **Neurons and Layers**\n",
        "* **Neurons:** The basic building blocks of a neural network are neurons, which are analogous to biological neurons in the brain. Each neuron takes inputs, processes them, and produces an output. In mathematical terms, a neuron performs a weighted sum of its inputs, applies an activation function, and produces an output.\n",
        "* **Example:** A single neuron might calculate the following:\n",
        "\n",
        "$$y = \\sigma(w_1x_1 + w_2x_2 + b)$$\n",
        "\n",
        "where $\\sigma$ is an activation function like ReLU or Sigmoid, w1, w2  are weights, x1, x2 are inputs, and b  is the bias.\n",
        "\n",
        "\n",
        "\n",
        "# Anatomy of a Neural Network\n",
        "\n",
        "* **Layers:** Neurons are grouped into layers. A typical neural network has an input layer, one or more hidden layers, and an output layer.\n",
        "  * **Input Layer:** Receives the input data.\n",
        "  * **Hidden Layers:** Perform computations and extract features. These layers can vary in number and size depending on the complexity of the task.\n",
        "  * **Output Layer:** Produces the final output of the network, such as class scores in a classification task.\n",
        "\n",
        "\n",
        "# Anatomy of a Neural Network\n",
        "\n",
        "* **Forward Pass**\n",
        "* **Definition:** The forward pass is the process where input data is passed through the network layer by layer, producing an output. This output is then compared with the ground truth to calculate the loss.\n",
        "\n",
        "\n",
        "* **Activation Functions**\n",
        "* **Role:** Activation functions introduce non-linearity into the model, allowing the network to learn complex patterns. Common activation functions include:\n",
        "* **ReLU (Rectified Linear Unit):** The most popular activation function, defined as\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max(0, x).$$\n",
        "\n",
        "* **Sigmoid:** Used in binary classification problems, defined as\n",
        "\n",
        "$$\\sigma(x) = \\frac{1}{1+e^{-x}}.$$\n",
        "\n",
        "* **Tanh:** Another activation function that outputs values between -1 and 1, defined as\n",
        "\n",
        "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YI_K8uy29Mgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 1. Creating Neural Networks with `torch.nn.Module`\n",
        "\n",
        "**Overview:** PyTorch provides the `torch.nn.Module` class, which is a base class for all neural network modules. It’s designed to encapsulate the structure and behavior of neural networks.\n",
        "\n",
        "### Defining a Neural Network Class\n",
        "\n",
        "* **Subclassing torch.nn.Module:** To define a neural network in PyTorch, you create a class that inherits from `torch.nn.Module`. This class must implement two methods: `__init__()` to define the network’s layers and `forward()` to define how the input data flows through the network.\n",
        "* **Example:**\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(10, 50)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(50, 1)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation after the first layer\n",
        "        x = self.fc2(x)              # Output layer\n",
        "        return x\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### Initializing and Using the Network\n",
        "\n",
        "* **Instantiation:** Once the network class is defined, you can instantiate it and pass data through it.\n",
        "* **Example:**\n",
        "```python\n",
        "model = SimpleNN()\n",
        "input_data = torch.randn(1, 10)  # Random input tensor with 10 features\n",
        "output = model(input_data)\n",
        "print(output)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Parameters:** The parameters of the model, such as **weights and biases**, are automatically registered and can be accessed using `model.parameters()`.\n",
        "\n",
        "### Managing Complexity\n",
        "\n",
        "* **Modular Design:** PyTorch encourages modular design, where complex networks are composed of smaller, reusable modules. For example, you can create custom layers by combining existing layers or by defining new layers that inherit from `torch.nn.Module`.\n",
        "\n",
        "\n",
        "## 2. Training a Neural Network: Forward Pass, Backward Pass\n",
        "\n",
        "Training a neural network involves a cycle of forward and backward passes, followed by updates to the model’s parameters.\n",
        "\n",
        "* **Forward Pass**\n",
        "* **Definition:** The forward pass involves passing the input data through the network to obtain predictions. This is the first step in the training process and is used to compute the loss.\n",
        "\n",
        "\n",
        "* **Backward Pass (Backpropagation)**\n",
        "* **Definition:** The backward pass involves computing the gradients of the loss with respect to each parameter of the network using backpropagation. This is done by calling the `backward()` method on the loss.\n",
        "* **Example:**\n",
        "```python\n",
        "loss.backward()  # Computes gradients for each parameter\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "* **Gradient Descent:** The gradients are then used to update the model’s parameters in the direction that reduces the loss, typically using an optimizer.\n",
        "\n",
        "\n",
        "* **Epochs and Iterations**\n",
        "* **Epoch:** One complete pass through the entire training dataset.\n",
        "* **Iteration:** One pass through a single batch of data. The number of iterations per epoch depends on the batch size and the size of the dataset.\n",
        "\n",
        "\n",
        "\n",
        "## 3. Loss Functions and Optimizers in PyTorch\n",
        "\n",
        "Loss functions and optimizers are key components in the training process. The loss function quantifies how well the model’s predictions match the target values, while the optimizer updates the model’s parameters to minimize this loss.\n",
        "\n",
        "### Loss Functions\n",
        "\n",
        "* **Definition:** A loss function measures the difference between the network’s predictions and the actual targets. PyTorch provides several loss functions for different types of tasks:\n",
        "* **MSE Loss (Mean Squared Error):** Used for regression tasks where the goal is to predict continuous values.\n",
        "* **Example:**\n",
        "```python\n",
        "loss_fn = nn.MSELoss()\n",
        "loss = loss_fn(predictions, targets)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Cross-Entropy Loss:** Commonly used in classification tasks where the target is a category.\n",
        "* **Example:**\n",
        "```python\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(predictions, targets)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Optimizers\n",
        "\n",
        "* **Definition:** An optimizer updates the network's parameters based on the computed gradients. PyTorch provides several optimizers, including:\n",
        "* **SGD (Stochastic Gradient Descent):** The most basic optimizer that updates parameters by moving them in the direction of the negative gradient.\n",
        "* **Example:**\n",
        "```python\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "* **Adam:** An advanced optimizer that adapts the learning rate for each parameter, often leading to faster convergence.\n",
        "* **Example:**\n",
        "```python\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Optimization Loop\n",
        "\n",
        "* **Step 1:** Zero the gradients to prevent gradient accumulation.\n",
        "* **Step 2:** Perform the forward pass and compute the loss.\n",
        "* **Step 3:** Perform the backward pass to compute gradients.\n",
        "* **Step 4:** Update the parameters using the optimizer.\n",
        "* **Example:**\n",
        "```python\n",
        "optimizer.zero_grad()           # Step 1\n",
        "output = model(input_data)      # Step 2\n",
        "loss = loss_fn(output, targets)\n",
        "loss.backward()                 # Step 3\n",
        "optimizer.step()                # Step 4\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v8pdYkx2-4eG"
      }
    }
  ]
}